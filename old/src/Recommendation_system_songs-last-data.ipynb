{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba7e3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, Dot, Add, Flatten, Dense, Concatenate\n",
    "from keras.layers import Dropout, BatchNormalization, Activation\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Embedding, Dot, Add, Flatten, Dense, Activation, Dropout, Concatenate\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c61f67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  user              song           timestamp  time_of_day\n",
      "0        user_007XIjOr  DaTQ53TUmfP93FSr 2019-02-20 12:28:00            2\n",
      "1        user_007XIjOr  dGeyvi5WCOjDU7da 2019-02-20 12:35:00            2\n",
      "2        user_007XIjOr  qUm54NYOjeFhmKYx 2019-02-20 12:48:00            2\n",
      "3        user_007XIjOr  FtnuMT1DlevSR2n5 2019-02-20 12:52:00            2\n",
      "4        user_007XIjOr  LHETTZcSZLeaVOGh 2019-02-20 13:09:00            2\n",
      "...                ...               ...                 ...          ...\n",
      "5109587  user_zzWscYTy  BBiswLufo26YQCT7 2019-01-10 15:57:00            2\n",
      "5109588  user_zzWscYTy  5ZHgff3sjETIiedr 2019-01-10 16:21:00            2\n",
      "5109589  user_zzWscYTy  m4O1iLh6fC43xjRy 2019-01-10 16:48:00            2\n",
      "5109590  user_zzWscYTy  mvUaP8k67qOFfA65 2019-01-10 21:13:00            3\n",
      "5109591  user_zzWscYTy  BBiswLufo26YQCT7 2019-01-10 21:16:00            3\n",
      "\n",
      "[5109592 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Classify time of day\n",
    "def classify_time_of_day(timestamp):\n",
    "    hour = timestamp.hour\n",
    "    if 4 <= hour < 12:\n",
    "        return 1\n",
    "    elif 12 <= hour < 20:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "# Load data\n",
    "metadata_path = '../data/id_metadata.csv'\n",
    "listening_history_path = '../data/listening_history.csv'\n",
    "metadata_df = pd.read_csv(metadata_path, delimiter='\\t')\n",
    "df = pd.read_csv(listening_history_path, delimiter='\\t')\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "metadata_df.rename(columns = {'id': 'song'}, inplace = True)\n",
    "df['time_of_day'] = df['timestamp'].apply(classify_time_of_day)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a032d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  user              song           timestamp  time_of_day\n",
      "5288     user_02jFGVkG  ke5JKpLa8Dw7dCDO 2019-03-19 12:39:00            2\n",
      "5289     user_02jFGVkG  nRpEFcFzYZ8Z8Cye 2019-03-19 12:44:00            2\n",
      "5290     user_02jFGVkG  UTDxdZ1outySsU7O 2019-03-19 15:18:00            2\n",
      "5291     user_02jFGVkG  UTDxdZ1outySsU7O 2019-03-19 15:24:00            2\n",
      "5292     user_02jFGVkG  bxo3drSzBGDlrodp 2019-03-19 15:24:00            2\n",
      "...                ...               ...                 ...          ...\n",
      "5104162  user_zwQunLVn  A8GhNkPLuKusMIvF 2019-03-21 18:29:00            2\n",
      "5104163  user_zwQunLVn  0m1MyuwLx1EjuQqD 2019-03-21 22:29:00            3\n",
      "5104164  user_zwQunLVn  0m1MyuwLx1EjuQqD 2019-03-21 22:33:00            3\n",
      "5104165  user_zwQunLVn  l2kISVY4j8iz1Gg7 2019-03-21 22:36:00            3\n",
      "5104166  user_zwQunLVn  l2kISVY4j8iz1Gg7 2019-03-21 22:40:00            3\n",
      "\n",
      "[68090 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Ensure the 'timestamp' column is datetime type\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Find the latest date in the data\n",
    "latest_date = df['timestamp'].max()\n",
    "\n",
    "# Calculate the date 14 days (2 weeks) prior to the latest date\n",
    "two_weeks_ago = latest_date - pd.Timedelta(days=7)\n",
    "\n",
    "# Filter the DataFrame to include only the last two weeks of data\n",
    "df = df[df['timestamp'] >= two_weeks_ago]\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5bfb47b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                user              song           timestamp  time_of_day  \\\n",
      "0      user_02jFGVkG  ke5JKpLa8Dw7dCDO 2019-03-19 12:39:00    -0.142363   \n",
      "1      user_2uotd4qv  ke5JKpLa8Dw7dCDO 2019-03-19 21:49:00     1.212425   \n",
      "2      user_McRU0r8i  ke5JKpLa8Dw7dCDO 2019-03-20 18:59:00    -0.142363   \n",
      "3      user_McRU0r8i  ke5JKpLa8Dw7dCDO 2019-03-20 19:41:00    -0.142363   \n",
      "4      user_02jFGVkG  nRpEFcFzYZ8Z8Cye 2019-03-19 12:44:00    -0.142363   \n",
      "...              ...               ...                 ...          ...   \n",
      "68085  user_zwQunLVn  VnF0TlGM24ZQaFxu 2019-03-21 11:02:00    -1.497152   \n",
      "68086  user_zwQunLVn  7ViolZVDtOxSjByn 2019-03-21 12:30:00    -0.142363   \n",
      "68087  user_zwQunLVn  Bk8GOanz19kxdO59 2019-03-21 18:02:00    -0.142363   \n",
      "68088  user_zwQunLVn  qUXY2aw3nHQ4Qslh 2019-03-21 18:06:00    -0.142363   \n",
      "68089  user_zwQunLVn  A8GhNkPLuKusMIvF 2019-03-21 18:29:00    -0.142363   \n",
      "\n",
      "        release  popularity  danceability    energy       key      mode  \\\n",
      "0     -1.482494   -0.369772     -1.819800 -0.602781 -0.916059 -1.250220   \n",
      "1     -1.482494   -0.369772     -1.819800 -0.602781 -0.916059 -1.250220   \n",
      "2     -1.482494   -0.369772     -1.819800 -0.602781 -0.916059 -1.250220   \n",
      "3     -1.482494   -0.369772     -1.819800 -0.602781 -0.916059 -1.250220   \n",
      "4     -1.482494    0.239512      0.021888  0.787531  0.482058 -1.250220   \n",
      "...         ...         ...           ...       ...       ...       ...   \n",
      "68085  0.630480   -0.065130     -0.226151 -0.940568 -1.195683  0.799859   \n",
      "68086  0.630480   -0.186987      0.567573  1.355406  1.041305 -1.250220   \n",
      "68087  0.630480   -0.369772      0.424951  1.272183  0.482058  0.799859   \n",
      "68088  0.342347   -0.369772     -0.288160  0.748368 -1.195683 -1.250220   \n",
      "68089  0.534436   -0.613486      0.641985  1.071469 -1.195683  0.799859   \n",
      "\n",
      "        valence     tempo  \n",
      "0     -1.036510 -1.660170  \n",
      "1     -1.036510 -1.660170  \n",
      "2     -1.036510 -1.660170  \n",
      "3     -1.036510 -1.660170  \n",
      "4      0.562468 -1.106867  \n",
      "...         ...       ...  \n",
      "68085 -0.877934  0.236975  \n",
      "68086  0.967719 -0.926846  \n",
      "68087  1.778221  0.341861  \n",
      "68088 -0.666499 -0.400794  \n",
      "68089  0.809143  1.010050  \n",
      "\n",
      "[68090 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.merge(df, metadata_df[['song', 'release', 'popularity', 'danceability', 'energy', 'key', 'mode', 'valence', 'tempo']], on='song')\n",
    "numeric_cols_df = df.select_dtypes(include=np.number).columns\n",
    "sscaler = StandardScaler()\n",
    "df[numeric_cols_df] = sscaler.fit_transform(df[numeric_cols_df])\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcbe2968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user            0\n",
      "song            0\n",
      "timestamp       0\n",
      "time_of_day     0\n",
      "release         0\n",
      "popularity      0\n",
      "danceability    0\n",
      "energy          0\n",
      "key             0\n",
      "mode            0\n",
      "valence         0\n",
      "tempo           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4e5f879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique_names_song: (18193,)\n",
      "unique_names_user: (633,)\n"
     ]
    }
   ],
   "source": [
    "unique_names_song = df.song.unique()\n",
    "unique_names_user = df.user.unique()\n",
    "\n",
    "print(\"unique_names_song:\", unique_names_song.shape)\n",
    "print(\"unique_names_user:\", unique_names_user.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f058f0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          user_id  song_id   release  popularity  danceability    energy  \\\n",
      "0               0        0 -1.482494   -0.369772     -1.819800 -0.602781   \n",
      "1               0        1 -1.482494    0.239512      0.021888  0.787531   \n",
      "2               0        2  0.534436   -0.796271      0.505564 -0.529349   \n",
      "3               0        3 -1.482494   -0.918128      0.629583 -0.940568   \n",
      "4               0        4 -1.482494   -0.857200      1.038847 -0.862241   \n",
      "...           ...      ...       ...         ...           ...       ...   \n",
      "11516164      632    18188  0.630480   -0.065130     -0.226151 -0.940568   \n",
      "11516165      632    18189  0.630480   -0.186987      0.567573  1.355406   \n",
      "11516166      632    18190  0.630480   -0.369772      0.424951  1.272183   \n",
      "11516167      632    18191  0.342347   -0.369772     -0.288160  0.748368   \n",
      "11516168      632    18192  0.534436   -0.613486      0.641985  1.071469   \n",
      "\n",
      "               key      mode   valence     tempo  time_of_day  interaction  \n",
      "0        -0.916059 -1.250220 -1.036510 -1.660170    -0.142363     0.000220  \n",
      "1         0.482058 -1.250220  0.562468 -1.106867     1.212425     0.000714  \n",
      "2        -0.077189 -1.250220  1.456663 -0.749077    -0.142363     0.000110  \n",
      "3        -0.077189  0.799859 -0.415419 -0.895721    -0.142363     0.000110  \n",
      "4        -0.356813 -1.250220  0.192457  0.248418    -0.142363     0.000110  \n",
      "...            ...       ...       ...       ...          ...          ...  \n",
      "11516164 -1.195683  0.799859 -0.877934  0.236975    -1.497152     0.000000  \n",
      "11516165  1.041305 -1.250220  0.967719 -0.926846    -0.142363     0.000000  \n",
      "11516166  0.482058  0.799859  1.778221  0.341861    -0.142363     0.000000  \n",
      "11516167 -1.195683 -1.250220 -0.666499 -0.400794    -0.142363     0.000000  \n",
      "11516168 -1.195683  0.799859  0.809143  1.010050    -0.142363     0.000000  \n",
      "\n",
      "[11516169 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "df.isnull().values.any()# Calculate song popularity\n",
    "song_popularity = df['song'].value_counts() / len(unique_names_song)\n",
    "df['song_popularity'] = df['song'].map(song_popularity)\n",
    "\n",
    "# Create an empty interaction matrix\n",
    "interaction_matrix = np.zeros((df['user'].nunique(), len(unique_names_song)))\n",
    "\n",
    "# Map users and songs to matrix indices\n",
    "user_indices = {user: idx for idx, user in enumerate(df['user'].unique())}\n",
    "song_indices = {song: idx for idx, song in enumerate(unique_names_song)}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    user_idx = user_indices[row['user']]\n",
    "    song_idx = song_indices[row['song']]\n",
    "    interaction_matrix[user_idx, song_idx] = np.log(row['song_popularity'] + 1)\n",
    "\n",
    "# Prepare dictionaries to map song IDs to their features\n",
    "song_features = {\n",
    "    'release': df.set_index('song')['release'].to_dict(),\n",
    "    'popularity': df.set_index('song')['popularity'].to_dict(),\n",
    "    'danceability': df.set_index('song')['danceability'].to_dict(),\n",
    "    'energy': df.set_index('song')['energy'].to_dict(),\n",
    "    'key': df.set_index('song')['key'].to_dict(),\n",
    "    'mode': df.set_index('song')['mode'].to_dict(),\n",
    "    'valence': df.set_index('song')['valence'].to_dict(),\n",
    "    'tempo': df.set_index('song')['tempo'].to_dict(),\n",
    "    'time_of_day': df.set_index('song')['time_of_day'].to_dict(),\n",
    "}\n",
    "\n",
    "# Create lists for DataFrame including additional features\n",
    "user_ids, song_ids, releases, popularities, danceabilities, energies, keys, modes, valences, tempos, interactions, time_of_days = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "for user in user_indices:\n",
    "    for song in song_indices:\n",
    "        user_ids.append(user_indices[user])\n",
    "        song_ids.append(song_indices[song])\n",
    "        interactions.append(interaction_matrix[user_indices[user], song_indices[song]])\n",
    "        # Map each song to its additional features\n",
    "        releases.append(song_features['release'][song])\n",
    "        popularities.append(song_features['popularity'][song])\n",
    "        danceabilities.append(song_features['danceability'][song])\n",
    "        energies.append(song_features['energy'][song])\n",
    "        keys.append(song_features['key'][song])\n",
    "        modes.append(song_features['mode'][song])\n",
    "        valences.append(song_features['valence'][song])\n",
    "        tempos.append(song_features['tempo'][song])\n",
    "        time_of_days.append(song_features['time_of_day'][song])\n",
    "\n",
    "# Create the interaction DataFrame\n",
    "interaction_df = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'song_id': song_ids,\n",
    "    'release': releases,\n",
    "    'popularity': popularities,\n",
    "    'danceability': danceabilities,\n",
    "    'energy': energies,\n",
    "    'key': keys,\n",
    "    'mode': modes,\n",
    "    'valence': valences,\n",
    "    'tempo': tempos,\n",
    "    'time_of_day': time_of_days,\n",
    "    'interaction': interactions\n",
    "})\n",
    "\n",
    "print(interaction_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef675d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users:  633\n",
      "Number of songs:  18193\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((68090, 15), (11516169, 12))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "song_encoder = LabelEncoder()\n",
    "df['user_id'] = user_encoder.fit_transform(df['user'])\n",
    "df['song_id'] = song_encoder.fit_transform(df['song'])\n",
    "\n",
    "N = df.user_id.nunique() # Number of users\n",
    "M = df.song_id.nunique() # Number of songs\n",
    "\n",
    "print(\"Number of users: \", N)\n",
    "print(\"Number of songs: \", M)\n",
    "df.shape, interaction_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1807db7f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          user_id  song_id   release  popularity  danceability    energy  \\\n",
      "3437593       188    17309  0.438392    0.300440      0.046692 -0.475499   \n",
      "10363724      569    11907  0.726525    0.483225      1.441910 -0.235621   \n",
      "6280692       345     4107 -3.979646   -1.649269      0.505564  0.860963   \n",
      "3531948       194     2506 -0.329963    0.178583     -0.219950  0.038525   \n",
      "4391169       241     6656  0.534436   -0.857200     -0.157940  0.346939   \n",
      "...           ...      ...       ...         ...           ...       ...   \n",
      "2234489       122    14943 -1.386450   -0.796271     -0.021519 -0.759436   \n",
      "4304572       236    11024 -0.329963   -0.735343      0.344338  1.174273   \n",
      "10081351      554     2429  0.438392   -0.186987      0.914828  0.581922   \n",
      "6550634       360     1154  0.726525   -0.186987      0.697794 -0.612572   \n",
      "6423388       353     1259  0.726525    0.361368     -0.933061  0.082584   \n",
      "\n",
      "               key      mode   valence     tempo  time_of_day  interaction  \n",
      "3437593   1.041305  0.799859 -0.208389 -0.920438    -1.497152          0.0  \n",
      "10363724  1.600552  0.799859 -1.327234  0.660358    -1.497152          0.0  \n",
      "6280692  -1.475306  0.799859  2.223116 -0.992721    -0.142363          0.0  \n",
      "3531948   0.482058  0.799859 -1.124608 -0.908326    -0.142363          0.0  \n",
      "4391169  -0.356813  0.799859 -1.115799 -0.889665     1.212425          0.0  \n",
      "...            ...       ...       ...       ...          ...          ...  \n",
      "2234489  -0.356813  0.799859 -1.758474 -1.664642    -0.142363          0.0  \n",
      "4304572  -0.356813 -1.250220  0.566873 -1.489303    -0.142363          0.0  \n",
      "10081351 -1.475306  0.799859  1.258443 -0.608525    -0.142363          0.0  \n",
      "6550634   1.041305  0.799859  0.302579 -1.459622    -1.497152          0.0  \n",
      "6423388  -0.077189  0.799859  1.350946  1.986983    -1.497152          0.0  \n",
      "\n",
      "[9212935 rows x 12 columns]           user_id  song_id   release  popularity  danceability    energy  \\\n",
      "5047202       277     7741 -2.635026   -0.674414      0.834215 -0.573408   \n",
      "5262088       289     4311 -1.866672   -0.796271      0.418750  0.856068   \n",
      "4163837       228    15833 -0.810184   -0.796271     -0.052524  1.037200   \n",
      "3238918       178      564 -0.714140    0.483225     -0.883453  1.081260   \n",
      "6800624       373    14635  0.726525   -1.832054      0.028089 -1.141283   \n",
      "...           ...      ...       ...         ...           ...       ...   \n",
      "4784400       262    17834 -0.906229   -0.918128      0.195515 -0.362903   \n",
      "5562505       305    13640  0.534436   -0.674414     -0.337768  1.443524   \n",
      "6560137       360    10657  0.342347   -0.552558      0.958234 -1.097223   \n",
      "8921768       490     7198  0.630480    1.214366      1.386101  0.846277   \n",
      "10146435      557    12934 -0.810184   -1.771126     -1.255512  0.371417   \n",
      "\n",
      "               key      mode   valence     tempo  time_of_day  interaction  \n",
      "5047202   0.202434 -1.250220  1.844294  0.474316     1.212425          0.0  \n",
      "5262088   0.482058 -1.250220 -0.300892 -0.047194     1.212425          0.0  \n",
      "4163837   1.041305 -1.250220  0.509610  0.169093     1.212425          0.0  \n",
      "3238918  -1.195683  0.799859  1.121891  1.615039    -1.497152          0.0  \n",
      "6800624  -0.077189  0.799859 -0.137911 -0.042159     1.212425          0.0  \n",
      "...            ...       ...       ...       ...          ...          ...  \n",
      "4784400  -0.916059  0.799859 -0.397800 -0.325483    -0.142363          0.0  \n",
      "5562505   1.600552 -1.250220 -0.358156  0.413687     1.212425          0.0  \n",
      "6560137   1.600552  0.799859 -1.027701  0.375837    -1.497152          0.0  \n",
      "8921768  -0.916059  0.799859  0.174837  2.136584    -0.142363          0.0  \n",
      "10146435 -1.195683  0.799859 -1.173062  0.659337    -1.497152          0.0  \n",
      "\n",
      "[2303234 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(interaction_df, test_size=0.2, random_state=42)\n",
    "print(df_train, df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342056a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unique interactions:  109\n"
     ]
    }
   ],
   "source": [
    "print(\"unique interactions: \", df_train.interaction.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "efd47681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           release  popularity  danceability    energy       key      mode  \\\n",
      "3437593   0.438392    0.300440      0.046692 -0.475499  1.041305  0.799859   \n",
      "10363724  0.726525    0.483225      1.441910 -0.235621  1.600552  0.799859   \n",
      "6280692  -3.979646   -1.649269      0.505564  0.860963 -1.475306  0.799859   \n",
      "3531948  -0.329963    0.178583     -0.219950  0.038525  0.482058  0.799859   \n",
      "4391169   0.534436   -0.857200     -0.157940  0.346939 -0.356813  0.799859   \n",
      "...            ...         ...           ...       ...       ...       ...   \n",
      "2234489  -1.386450   -0.796271     -0.021519 -0.759436 -0.356813  0.799859   \n",
      "4304572  -0.329963   -0.735343      0.344338  1.174273 -0.356813 -1.250220   \n",
      "10081351  0.438392   -0.186987      0.914828  0.581922 -1.475306  0.799859   \n",
      "6550634   0.726525   -0.186987      0.697794 -0.612572  1.041305  0.799859   \n",
      "6423388   0.726525    0.361368     -0.933061  0.082584 -0.077189  0.799859   \n",
      "\n",
      "           valence     tempo  time_of_day  \n",
      "3437593  -0.208389 -0.920438    -1.497152  \n",
      "10363724 -1.327234  0.660358    -1.497152  \n",
      "6280692   2.223116 -0.992721    -0.142363  \n",
      "3531948  -1.124608 -0.908326    -0.142363  \n",
      "4391169  -1.115799 -0.889665     1.212425  \n",
      "...            ...       ...          ...  \n",
      "2234489  -1.758474 -1.664642    -0.142363  \n",
      "4304572   0.566873 -1.489303    -0.142363  \n",
      "10081351  1.258443 -0.608525    -0.142363  \n",
      "6550634   0.302579 -1.459622    -1.497152  \n",
      "6423388   1.350946  1.986983    -1.497152  \n",
      "\n",
      "[9212935 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "continuous_data_train = df_train.iloc[:,2:-1]\n",
    "continuous_data_test = df_test.iloc[:,2:-1]\n",
    "continuous_data_train.shape, continuous_data_test.shape, df_train.shape\n",
    "print(continuous_data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c50d01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 'model.h5' already exists. Loading model...\n"
     ]
    }
   ],
   "source": [
    "K = 15 # define the size of embeddings, capture the relations in data (10-50)\n",
    "\n",
    "mu = df_train.interaction.mean()  # Mean interaction for normalization\n",
    "epochs = 1\n",
    "model_path = '../helpers/model.h5'\n",
    "if not os.path.exists(model_path):\n",
    "    u = Input(shape=(1,))\n",
    "    s = Input(shape=(1,))\n",
    "    u_embedding = Embedding(N, K)(u) # (N, 1, K)\n",
    "    s_embedding = Embedding(M, K)(s) # (N, 1, K)\n",
    "\n",
    "\n",
    "    ##### main branch\n",
    "    u_bias = Embedding(N, 1)(u) # (N, 1, 1)\n",
    "    s_bias = Embedding(M, 1)(s) # (N, 1, 1)\n",
    "    x = Dot(axes=2)([u_embedding, s_embedding]) # (N, 1, 1)\n",
    "    x = Add()([x, u_bias, s_bias])\n",
    "    x = Flatten()(x) # (N, 1)\n",
    "\n",
    "    # CONTINUOUS BRANCH\n",
    "    continuous_input = Input(shape=(continuous_data_train.shape[1],))\n",
    "\n",
    "\n",
    "    ##### side branch\n",
    "    u_embedding = Flatten()(u_embedding) # (N, K)\n",
    "    s_embedding = Flatten()(s_embedding) # (N, K)\n",
    "    y = Concatenate()([u_embedding, s_embedding, continuous_input]) # (N, 2K)\n",
    "    y = Dense(512)(y)\n",
    "    y = Activation('elu')(y)\n",
    "    y = Dropout(0.3)(y)\n",
    "    y = Dense(512)(y)\n",
    "    y = Activation('elu')(y)\n",
    "    y = Dropout(0.3)(y)\n",
    "    y = Dense(512)(y)\n",
    "    y = Activation('elu')(y)\n",
    "    y = Dropout(0.3)(y)\n",
    "    y = Dense(1)(y)\n",
    "\n",
    "\n",
    "    ##### merge\n",
    "    x = Add()([x, y])\n",
    "\n",
    "    model = Model(inputs=[u, s, continuous_input], outputs=x)\n",
    "    model.compile(\n",
    "      loss='mse',\n",
    "      # optimizer='adam',\n",
    "      #optimizer=Adam(lr=0.001),\n",
    "      optimizer=SGD(learning_rate=0.08, momentum=0.9),\n",
    "      metrics=['mse'],\n",
    "    )\n",
    "\n",
    "    # Now, train the model\n",
    "    r = model.fit(\n",
    "      x=[df_train.user_id.values, df_train.song_id.values, continuous_data_train.values],\n",
    "      y=df_train.interaction.values - mu,\n",
    "      epochs=epochs,\n",
    "      batch_size=128,\n",
    "      validation_data=(\n",
    "        [df_test.user_id.values, df_test.song_id.values, continuous_data_test.values],\n",
    "        df_test.interaction.values - mu\n",
    "      )\n",
    "    )\n",
    "\n",
    "    plt.plot(r.history['loss'], label=\"train loss\")\n",
    "    plt.plot(r.history['val_loss'], label=\"test loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # plot mse\n",
    "    plt.plot(r.history['mse'], label=\"train mse\")\n",
    "    plt.plot(r.history['val_mse'], label=\"test mse\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    model.save(model_path)\n",
    "    print(\"Model trained and saved as 'model.h5'.\")\n",
    "else:\n",
    "    print(\"Model 'model.h5' already exists. Loading model...\")\n",
    "    # Load and use the existing model\n",
    "    model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbb6987c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 15)        9495        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 1, 15)        272895      ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 15)           0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 15)           0           ['embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 9)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 39)           0           ['flatten_1[0][0]',              \n",
      "                                                                  'flatten_2[0][0]',              \n",
      "                                                                  'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512)          20480       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 512)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 512)          0           ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 512)          262656      ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 512)          0           ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 512)          0           ['activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 512)          262656      ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " dot (Dot)                      (None, 1, 1)         0           ['embedding[0][0]',              \n",
      "                                                                  'embedding_1[0][0]']            \n",
      "                                                                                                  \n",
      " embedding_2 (Embedding)        (None, 1, 1)         633         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_3 (Embedding)        (None, 1, 1)         18193       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 512)          0           ['dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 1, 1)         0           ['dot[0][0]',                    \n",
      "                                                                  'embedding_2[0][0]',            \n",
      "                                                                  'embedding_3[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 512)          0           ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1)            0           ['add[0][0]']                    \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 1)            513         ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 1)            0           ['flatten[0][0]',                \n",
      "                                                                  'dense_3[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 847,521\n",
      "Trainable params: 847,521\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "No training session to plot, model was loaded from file.\n"
     ]
    }
   ],
   "source": [
    "model.summary()\n",
    "# Plot training and validation loss and MSE\n",
    "if 'r' in locals():  # Check if training results are available\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(r.history['loss'], label=\"train loss\")\n",
    "    plt.plot(r.history['val_loss'], label=\"test loss\")\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(r.history['mse'], label=\"train mse\")\n",
    "    plt.plot(r.history['val_mse'], label=\"test mse\")\n",
    "    plt.title('Mean Squared Error')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('MSE')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training session to plot, model was loaded from file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d5970bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 1s 1ms/step\n",
      "Top 10 recommended song IDs for user 277 are: ['yDPIzYrb35kO30gd' '5pMLWikvbUlxbw3C' 'VgCslSDAzOerZhTp'\n",
      " 'BBiswLufo26YQCT7' 'VFz60KPCOCrY9TNy' 'I0xepbeRWgWpFSyH'\n",
      " 'rnmOxlvA87MrxnHZ' 'UJj71KVEwKdqGsYr' 'dlQMazvCZtdUc4gp'\n",
      " '0GohjZ4zTigF1LeU']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "user_ids_test = df_test.user_id.values\n",
    "song_ids_test = df_test.song_id.values\n",
    "continuous_features = [\"release\", \"popularity\", \"danceability\", \"energy\", \"key\", \"mode\", \"valence\", \"tempo\", \"time_of_day\"]\n",
    "\n",
    "# Select a specific user for the prediction\n",
    "specific_user_id = user_ids_test[0]  # Example: taking the first user in the test set\n",
    "\n",
    "# Prepare input data for the model\n",
    "M = interaction_df['song_id'].nunique()  # Total number of unique songs\n",
    "user_input = np.array([specific_user_id] * M)  # Repeat the user ID for each song\n",
    "song_input = np.array(range(M))  # Array of all unique song IDs\n",
    "\n",
    "# Map song IDs to indices in df\n",
    "song_id_to_index = {id: idx for idx, id in enumerate(interaction_df['song_id'].unique())}\n",
    "\n",
    "# Prepare continuous data for all songs\n",
    "continuous_data_input = np.array([interaction_df.loc[song_id_to_index[song_id], continuous_features] for song_id in song_input])\n",
    "\n",
    "# Make predictions for this user with all songs, including the continuous data\n",
    "predicted_interactions = model.predict([user_input, song_input, continuous_data_input])\n",
    "\n",
    "\n",
    "# Convert predictions back to the original scale, if needed\n",
    "mu = df_train.interaction.mean()  # Mean interaction value for normalization (if used during training)\n",
    "predicted_interactions = predicted_interactions.flatten() + mu\n",
    "\n",
    "# Determine the number of top recommendations, e.g., top 10\n",
    "N = 10\n",
    "top_n_indices = np.argsort(predicted_interactions)[::-1][:N]\n",
    "\n",
    "# Convert the indices to original song IDs\n",
    "top_n_song_ids = song_encoder.inverse_transform(top_n_indices)\n",
    "\n",
    "# Output the recommended songs\n",
    "print(f\"Top {N} recommended song IDs for user {specific_user_id} are:\", top_n_song_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c831d4b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_input.shape:  (18193,)\n",
      "continuous_data_input.shape:  (18193, 9)\n"
     ]
    }
   ],
   "source": [
    "print(\"user_input.shape: \", user_input.shape)\n",
    "print(\"continuous_data_input.shape: \", continuous_data_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a614c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      id                           artist  \\\n",
      "0       0009fFIM1eYThaPg                           Cheryl   \n",
      "1       0010xmHR6UICBOYT                          Oddisee   \n",
      "2       002Jyd0vN4HyCpqL                 Blue Öyster Cult   \n",
      "3       006TYKNjNxWjfKjy                         Rhapsody   \n",
      "4       007LIJOPQ4Sb98qV                   The Chameleons   \n",
      "...                  ...                              ...   \n",
      "109264  zzyyPUs7hC9Nz2e1  Frank Carter & The Rattlesnakes   \n",
      "109265  zzz0n04uuTUA7fNh                    Justin Bieber   \n",
      "109266  zzzj3LYaZtYtbzSr                   Regina Spektor   \n",
      "109267  zzznMjZAKnJJXQSj                         Dua Lipa   \n",
      "109268  zzzwh2ktIWjsR7xp                      Snow Patrol   \n",
      "\n",
      "                                  song  \\\n",
      "0                           Rain on Me   \n",
      "1                       After Thoughts   \n",
      "2                               ME 262   \n",
      "3                    Flames of Revenge   \n",
      "4                            Nostalgia   \n",
      "...                                ...   \n",
      "109264                        Vampires   \n",
      "109265                    Heartbreaker   \n",
      "109266      Après Moi - Live In London   \n",
      "109267  New Rules (Initial Talk Remix)   \n",
      "109268                      In the End   \n",
      "\n",
      "                                               album_name  \n",
      "0                                                 3 Words  \n",
      "1                                       The Beauty in All  \n",
      "2                                         Secret Treaties  \n",
      "3                           Legendary Years (Re-Recorded)  \n",
      "4       What Does Anything Mean? Basically (2009 Remas...  \n",
      "...                                                   ...  \n",
      "109264                                        Modern Ruin  \n",
      "109265                                           Journals  \n",
      "109266                                     Live In London  \n",
      "109267                     New Rules (Initial Talk Remix)  \n",
      "109268                                     Fallen Empires  \n",
      "\n",
      "[109269 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "id_information = pd.read_csv('../data/id_information.csv', sep='\\t')\n",
    "print(id_information)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd42f672",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The user with ID 277 has interacted with 24 unique songs.\n",
      "['RCGLlYCigkopKd7D' 'wG3YQktIKVlZruUu' 'J4tQCUzKKKj3OkV0'\n",
      " 'AWLAvZOhu4vLV6RA' 'b5ef6NhN3RBrQXIE' 'SAryWbXDkmkFWW3Z'\n",
      " 'GQoNBkAIX2obbtPn' 'Ax8Us7NbJqe08R0a' 'Br8x0lv4QnA6pnqC'\n",
      " 'ilrbbVMFVbnwmdFp' 'rPcvhlfan3sOTDX8' '6WlGmqDauWacBf9D'\n",
      " 'vZ3hXeIO65MA9fBi' 's7f6VKariF11TTjK' 'aAXwzXQZdbdPxFqT'\n",
      " 'L6AP1Ype1fRuWBwD' '5yY4kVyuJPdbOegL' '2HgkT8eEkxxEo2Tg'\n",
      " 'bhOXqQHAGOeJqm6I' 'X8ZpmMJo8zT2QvcT' 'MwpFtrA5qYK4v5tl'\n",
      " '6U91Xztu28VOYkmU' 'NsNXlxbPuFnyPv9p' 'OBkwaSaAAVVcBaag']\n"
     ]
    }
   ],
   "source": [
    "unique_songs = df[df.user_id == specific_user_id].song.unique()\n",
    "\n",
    "# Print a user-friendly message\n",
    "print(f\"The user with ID {specific_user_id} has interacted with {len(unique_songs)} unique songs.\")\n",
    "\n",
    "# Print a subset of the songs for brevity\n",
    "print(unique_songs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3674269a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.999985323305133\n",
      "\n",
      "\n",
      "4.999985226451906\n",
      "\n",
      "\n",
      "4.999986362778417\n",
      "\n",
      "\n",
      "4.999986066450398\n",
      "\n",
      "\n",
      "4.999986040665865\n",
      "\n",
      "\n",
      "4.999981166694702\n",
      "\n",
      "\n",
      "4.999985515421169\n",
      "\n",
      "\n",
      "4.999985078380071\n",
      "\n",
      "\n",
      "4.999985329503232\n",
      "\n",
      "\n",
      "4.999920069245894\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4.999986362778417 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Example song features DataFrame\n",
    "# Assume df_features is a DataFrame with songs as rows and features as columns\n",
    "\n",
    "# Calculate similarity\n",
    "def calculate_similarity(target_song_features, songs_features):\n",
    "    similarity = cosine_similarity([target_song_features], songs_features)\n",
    "    return similarity[0]  # similarity[0] because the result is in a 2D array\n",
    "\n",
    "maxi = -1\n",
    "best_idx = 0\n",
    "last_5_song_ids = list()\n",
    "\n",
    "#REWORKa\n",
    "last_5_song_ids = ['aPPVq97XeQv8mqsU', 'SSA3WorrB4G8ww60', 'lJKIbZNzpS6IsNgh', 'd8QDyWffh9zwJ4Gs', 'oLHuLrmZyV1oEfdu']\n",
    "#print(last_5_song_ids)\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    recommended_song_id = top_n_song_ids[i]\n",
    "    # Get the feature vector for the recommended song\n",
    "    recommended_song_features = interaction_df.loc[interaction_df.song_id[song_indices[recommended_song_id]]]\n",
    "\n",
    "    # Get the feature vectors for the last 5 played songs\n",
    "    last_5_songs_features = np.array([interaction_df.loc[interaction_df.song_id[song_indices[song_id]]] for song_id in last_5_song_ids])\n",
    "\n",
    "    # Calculate similarity\n",
    "    similarities = calculate_similarity(np.array(recommended_song_features).reshape(1, -1)[0], last_5_songs_features)\n",
    "    print(np.sum(similarities), end = \"\\n\\n\\n\")\n",
    "    \n",
    "    if np.sum(similarities) > maxi:\n",
    "        maxi = np.sum(similarities)\n",
    "        best_idx = i\n",
    "print()\n",
    "print()\n",
    "print(maxi, best_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "941936c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>artist</th>\n",
       "      <th>song</th>\n",
       "      <th>album_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>55951</th>\n",
       "      <td>VgCslSDAzOerZhTp</td>\n",
       "      <td>Pearl Jam</td>\n",
       "      <td>Life Wasted</td>\n",
       "      <td>Pearl Jam</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id     artist         song album_name\n",
       "55951  VgCslSDAzOerZhTp  Pearl Jam  Life Wasted  Pearl Jam"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommended_song_id = top_n_song_ids[best_idx]\n",
    "id_information[id_information.id == recommended_song_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "66c23bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id             artist                  song  \\\n",
      "50240  SSA3WorrB4G8ww60          blink-182  All the Small Things   \n",
      "64094  aPPVq97XeQv8mqsU        Fleet Foxes               Mykonos   \n",
      "68866  d8QDyWffh9zwJ4Gs           The 1975         Be My Mistake   \n",
      "83374  lJKIbZNzpS6IsNgh  Foster the People       Pumped Up Kicks   \n",
      "88650  oLHuLrmZyV1oEfdu           The 1975                  Mine   \n",
      "\n",
      "                                      album_name  \n",
      "50240                         Enema Of The State  \n",
      "64094                 First Collection 2006-2009  \n",
      "68866  A Brief Inquiry Into Online Relationships  \n",
      "83374                                    Torches  \n",
      "88650  A Brief Inquiry Into Online Relationships  \n"
     ]
    }
   ],
   "source": [
    "last_5_songs_info = id_information[id_information['id'].isin(last_5_song_ids)]\n",
    "\n",
    "# Display the information of the last 5 songs\n",
    "print(last_5_songs_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a27a52f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 1s 1ms/step\n",
      "Precision: 0.0000, Recall: 0.0000, F1 Score: 0.0000\n",
      "Precision@10: 0.0000\n",
      "Recall@10: 0.0000\n",
      "MAP@10: 0.0000\n",
      "MRR: 0.0000\n",
      "NDCG@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# def get_relevant_songs(user_id, df):\n",
    "#     \"\"\"\n",
    "#     Get a list of relevant songs for a given user.\n",
    "#     This function needs to be adapted based on how relevance is defined in your dataset.\n",
    "#     \"\"\"\n",
    "#     # Example: get songs that the user has interacted with\n",
    "#     return df[df.user_id == user_id]['song'].unique()\n",
    "\n",
    "# # Select a user and predict top N songs\n",
    "# specific_user_id = user_ids_test[0]\n",
    "# predicted_interactions = model.predict([user_input, song_input, continuous_data_input])\n",
    "# flattened_arr = predicted_interactions.flatten()\n",
    "\n",
    "# # Sort the array in descending order\n",
    "# sorted_indices = np.argsort(flattened_arr)[::-1]\n",
    "# sorted_arr = flattened_arr[sorted_indices]\n",
    "\n",
    "# # Get actual relevant songs for the user\n",
    "# actual_relevant_songs = get_relevant_songs(specific_user_id, df)\n",
    "\n",
    "# # Convert actual relevant songs and top recommended songs to a binary format\n",
    "# actual_binary = [1 if song_encoder.inverse_transform([song_id])[0] in actual_relevant_songs else 0 for song_id in song_ids_test]\n",
    "# predicted_binary = [1 if song_id in sorted_indices[:len(actual_relevant_songs)].tolist() else 0 for song_id in song_ids_test]\n",
    "\n",
    "# # Calculate precision, recall, and F1 score\n",
    "# precision = precision_score(actual_binary, predicted_binary)\n",
    "# recall = recall_score(actual_binary, predicted_binary)\n",
    "# f1 = f1_score(actual_binary, predicted_binary)\n",
    "\n",
    "# print(f\"Precision: {precision}, Recall: {recall}, F1 Score: {f1}\")\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "def precision_at_k(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(k)\n",
    "    return result\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    act_set = set(actual)\n",
    "    pred_set = set(predicted[:k])\n",
    "    result = len(act_set & pred_set) / float(len(act_set))\n",
    "    return result\n",
    "\n",
    "def mean_average_precision(actual, predicted, k):\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    for i, p in enumerate(predicted[:k]):\n",
    "        if p in actual and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    return score / min(len(actual), k)\n",
    "\n",
    "def mean_reciprocal_rank(relevant_results):\n",
    "    for index, score in enumerate(relevant_results):\n",
    "        if score:\n",
    "            return 1.0 / (index + 1)\n",
    "    return 0\n",
    "\n",
    "def ndcg_at_k(actual, predicted, k):\n",
    "    def dcg_at_k(scores, k):\n",
    "        return sum([score / np.log2(idx + 2) for idx, score in enumerate(scores[:k])])\n",
    "\n",
    "    actual_scores = [1 if song in actual else 0 for song in predicted[:k]]\n",
    "    best_scores = sorted(actual_scores, reverse=True)\n",
    "    ideal_dcg = dcg_at_k(best_scores, k)\n",
    "    actual_dcg = dcg_at_k(actual_scores, k)\n",
    "    if not ideal_dcg:\n",
    "        return 0.0\n",
    "    return actual_dcg / ideal_dcg\n",
    "\n",
    "def get_relevant_songs(user_id, df):\n",
    "    \"\"\"\n",
    "    Get a list of relevant songs for a given user.\n",
    "    This function needs to be adapted based on how relevance is defined in your dataset.\n",
    "    \"\"\"\n",
    "    return df[df.user_id == user_id]['song'].unique()\n",
    "\n",
    "# Select a user and predict top N songs\n",
    "specific_user_id = user_ids_test[0]\n",
    "predicted_interactions = model.predict([user_input, song_input, continuous_data_input])\n",
    "flattened_arr = predicted_interactions.flatten()\n",
    "\n",
    "# Sort the array in descending order\n",
    "sorted_indices = np.argsort(flattened_arr)[::-1]\n",
    "sorted_arr = flattened_arr[sorted_indices]\n",
    "\n",
    "# Get actual relevant songs for the user\n",
    "actual_relevant_songs = get_relevant_songs(specific_user_id, df)\n",
    "\n",
    "# Convert actual relevant songs and top recommended songs to a binary format\n",
    "actual_binary = [1 if song_encoder.inverse_transform([song_id])[0] in actual_relevant_songs else 0 for song_id in song_ids_test]\n",
    "predicted_binary = [1 if song_id in sorted_indices[:len(actual_relevant_songs)].tolist() else 0 for song_id in song_ids_test]\n",
    "\n",
    "# Calculate precision, recall, and F1 score\n",
    "precision = precision_score(actual_binary, predicted_binary)\n",
    "recall = recall_score(actual_binary, predicted_binary)\n",
    "f1 = f1_score(actual_binary, predicted_binary)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Top-k calculations\n",
    "k = 10  # Set this based on your analysis needs\n",
    "precision_k = precision_at_k(actual_relevant_songs, sorted_indices, k)\n",
    "recall_k = recall_at_k(actual_relevant_songs, sorted_indices, k)\n",
    "map_k = mean_average_precision(actual_relevant_songs, sorted_indices, k)\n",
    "mrr = mean_reciprocal_rank([1 if song_id in actual_relevant_songs else 0 for song_id in sorted_indices])\n",
    "ndcg_k = ndcg_at_k(actual_relevant_songs, sorted_indices, k)\n",
    "\n",
    "# Output the additional metrics\n",
    "print(f\"Precision@{k}: {precision_k:.4f}\")\n",
    "print(f\"Recall@{k}: {recall_k:.4f}\")\n",
    "print(f\"MAP@{k}: {map_k:.4f}\")\n",
    "print(f\"MRR: {mrr:.4f}\")\n",
    "print(f\"NDCG@{k}: {ndcg_k:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a24814b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# relevant = list()\n",
    "# for song_id in actual_relevant_songs:\n",
    "#     relevant.append(song_indices[song_id])\n",
    "# relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202fcef5-a23c-42b7-a4b6-6bc08f5836eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a655a-6ee7-4192-b963-dd7713869ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30d4f990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "569/569 [==============================] - 1s 1ms/step\n",
      "569/569 [==============================] - 1s 1ms/step\n",
      "569/569 [==============================] - 1s 1ms/step\n",
      "569/569 [==============================] - 1s 1ms/step\n",
      "569/569 [==============================] - 1s 1ms/step\n",
      "569/569 [==============================] - 1s 1ms/step\n",
      "569/569 [==============================] - 1s 1ms/step\n",
      "569/569 [==============================] - 1s 1ms/step\n",
      "569/569 [==============================] - 1s 1ms/step\n",
      "569/569 [==============================] - 1s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "# # Placeholder for storing predictions for all users\n",
    "# predictions = {}\n",
    "# actual = {}\n",
    "\n",
    "# # Get predictions and actual relevant songs for each user\n",
    "# for user_id in user_ids_test[:10]:  # Limit to first 10 users for the example\n",
    "#     # Predict top N songs\n",
    "#     user_input = np.array([user_id] * M)\n",
    "#     continuous_data_input = np.array([interaction_df.loc[song_id_to_index[song_id], continuous_features] for song_id in song_input])\n",
    "#     predicted_interactions = model.predict([user_input, song_input, continuous_data_input]).flatten()\n",
    "#     top_n_indices = np.argsort(predicted_interactions)[::-1][:N]\n",
    "#     top_n_song_ids = song_encoder.inverse_transform(top_n_indices)\n",
    "    \n",
    "#     # Store the top N song IDs for each user\n",
    "#     predictions[user_id] = top_n_song_ids.tolist()\n",
    "    \n",
    "#     # Get actual relevant songs for the user\n",
    "#     actual_relevant_songs = get_relevant_songs(user_id, df)\n",
    "#     actual[user_id] = actual_relevant_songs.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7b5a8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def precision_at_k(actual, predicted, k):\n",
    "#     precision_scores = []\n",
    "#     for user_id in actual:\n",
    "#         # Initialize true positives count\n",
    "#         true_positives = 0\n",
    "#         # Check if user exists in predictions\n",
    "#         if user_id in predicted and len(predicted[user_id]) >= k:\n",
    "#             # Count the number of relevant items in the top k predictions\n",
    "#             true_positives = len(set(predicted[user_id][:k]) & set(actual[user_id]))\n",
    "#         # Calculate precision for this user\n",
    "#         precision = true_positives / float(k)\n",
    "#         precision_scores.append(precision)\n",
    "#     # Return the average precision at k for all users\n",
    "#     return sum(precision_scores) / len(precision_scores)\n",
    "\n",
    "# def recall_at_k(actual, predicted, k):\n",
    "#     recall_scores = []\n",
    "#     for user_id in actual:\n",
    "#         # Initialize true positives count\n",
    "#         true_positives = 0\n",
    "#         # Check if user exists in predictions\n",
    "#         if user_id in predicted:\n",
    "#             # Count the number of relevant items in the top k predictions\n",
    "#             true_positives = len(set(predicted[user_id][:k]) & set(actual[user_id]))\n",
    "#             recall = true_positives / float(len(actual[user_id]))\n",
    "#         else:\n",
    "#             # If no predictions for the user, recall is 0\n",
    "#             recall = 0.0\n",
    "#         recall_scores.append(recall)\n",
    "#     # Return the average recall at k for all users\n",
    "#     return sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "# def avg_precision_at_k(actual, predicted, k=10):\n",
    "#     ap_sum = 0\n",
    "#     for user, true_items in actual.items():\n",
    "#         pred_items = predicted[user][:k]\n",
    "#         hits = 0\n",
    "#         sum_precs = 0\n",
    "#         for i, p in enumerate(pred_items):\n",
    "#             if p in true_items:\n",
    "#                 hits += 1\n",
    "#                 sum_precs += hits / (i + 1.0)\n",
    "#         ap_sum += sum_precs / min(len(true_items), k)\n",
    "#     return ap_sum / len(actual)\n",
    "\n",
    "# def mean_avg_precision_at_k(actual, predicted, k=10):\n",
    "#     return avg_precision_at_k(actual, predicted, k)\n",
    "\n",
    "# def mean_average_precision_at_k(actual, predicted, k=10):\n",
    "#     AP_sum = 0.0\n",
    "#     for user_id in actual:\n",
    "#         if user_id in predicted:\n",
    "#             pred_items = predicted[user_id][:k]\n",
    "#             hits = 0\n",
    "#             sum_precisions = 0\n",
    "#             for i, p in enumerate(pred_items):\n",
    "#                 if p in actual[user_id] and p not in pred_items[:i]:\n",
    "#                     hits += 1\n",
    "#                     sum_precisions += hits / (i + 1.0)\n",
    "#             AP_sum += sum_precisions / min(len(actual[user_id]), k)\n",
    "#     return AP_sum / len(actual)\n",
    "\n",
    "# def mean_reciprocal_rank(actual, predicted):\n",
    "#     MRR_sum = 0.0\n",
    "#     for user_id in actual:\n",
    "#         if user_id in predicted:\n",
    "#             pred_items = predicted[user_id]\n",
    "#             for rank, p in enumerate(pred_items, start=1):\n",
    "#                 if p in actual[user_id]:\n",
    "#                     MRR_sum += 1.0 / rank\n",
    "#                     break\n",
    "#     return MRR_sum / len(actual)\n",
    "\n",
    "# def dcg_at_k(relevances, k):\n",
    "#     relevances = np.asfarray(relevances)[:k]\n",
    "#     if relevances.size:\n",
    "#         return np.sum(relevances / np.log2(np.arange(2, relevances.size + 2)))\n",
    "#     return 0.0\n",
    "\n",
    "# def ndcg_at_k(actual, predicted, k=10):\n",
    "#     NDCG_sum = 0.0\n",
    "#     for user_id in actual:\n",
    "#         if user_id in predicted:\n",
    "#             pred_items = predicted[user_id][:k]\n",
    "#             true_relevances = [1 if item in actual[user_id] else 0 for item in pred_items]\n",
    "#             ideal_relevances = [1] * len(actual[user_id])\n",
    "#             NDCG_sum += dcg_at_k(true_relevances, k) / dcg_at_k(ideal_relevances, k)\n",
    "#     return NDCG_sum / len(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ae46a373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0000\n",
      "Recall@10: 0.0000\n",
      "MAP@10: 0.0000\n",
      "MRR: 0.0000\n",
      "NDCG@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# k = 10  # You can change this value based on how many recommendations you consider\n",
    "# precision = precision_at_k(actual, predictions, k)\n",
    "# recall = recall_at_k(actual, predictions, k)\n",
    "# map_k = mean_average_precision_at_k(actual, predictions, k)\n",
    "# mrr = mean_reciprocal_rank(actual, predictions)\n",
    "# ndcg_k = ndcg_at_k(actual, predictions, k)\n",
    "\n",
    "# print(f\"Precision@{k}: {precision:.4f}\")\n",
    "# print(f\"Recall@{k}: {recall:.4f}\")\n",
    "# print(f\"MAP@{k}: {map_k:.4f}\")\n",
    "# print(f\"MRR: {mrr:.4f}\")\n",
    "# print(f\"NDCG@{k}: {ndcg_k:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c2dc265d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@10: 0.0000\n",
      "Recall@10: 0.0000\n",
      "MAP@10: 0.0000\n",
      "MRR: 0.0000\n",
      "NDCG@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# def precision_at_k(actual, predicted, k):\n",
    "#     act_set = set(actual)\n",
    "#     pred_set = set(predicted[:k])\n",
    "#     result = len(act_set & pred_set) / float(k)\n",
    "#     return result\n",
    "\n",
    "# def recall_at_k(actual, predicted, k):\n",
    "#     act_set = set(actual)\n",
    "#     pred_set = set(predicted[:k])\n",
    "#     result = len(act_set & pred_set) / float(len(act_set))\n",
    "#     return result\n",
    "\n",
    "# def mean_average_precision(actual, predicted, k):\n",
    "#     score = 0.0\n",
    "#     num_hits = 0.0\n",
    "#     for i, p in enumerate(predicted[:k]):\n",
    "#         if p in actual and p not in predicted[:i]:\n",
    "#             num_hits += 1.0\n",
    "#             score += num_hits / (i + 1.0)\n",
    "#     return score / min(len(actual), k)\n",
    "\n",
    "# def mean_reciprocal_rank(relevant_results):\n",
    "#     for index, score in enumerate(relevant_results):\n",
    "#         if score:\n",
    "#             return 1.0 / (index + 1)\n",
    "#     return 0\n",
    "\n",
    "# def ndcg_at_k(actual, predicted, k):\n",
    "#     def dcg_at_k(scores, k):\n",
    "#         return sum([score / np.log2(idx + 2) for idx, score in enumerate(scores[:k])])\n",
    "\n",
    "#     actual_scores = [1 if song in actual else 0 for song in predicted[:k]]\n",
    "#     best_scores = sorted(actual_scores, reverse=True)\n",
    "#     ideal_dcg = dcg_at_k(best_scores, k)\n",
    "#     actual_dcg = dcg_at_k(actual_scores, k)\n",
    "#     if not ideal_dcg:\n",
    "#         return 0.0\n",
    "#     return actual_dcg / ideal_dcg\n",
    "\n",
    "# # Implement your song prediction and evaluation logic here\n",
    "# # For example:\n",
    "# # specific_user_id = user_ids_test[0]\n",
    "# # predicted_interactions = model.predict([user_input, song_input, continuous_data_input])\n",
    "# # flattened_arr = predicted_interactions.flatten()\n",
    "\n",
    "# # Calculate the ranking metrics\n",
    "# k = 10  # Top-k items\n",
    "# predicted_top_k = sorted_indices[:k]\n",
    "# precision = precision_at_k(actual_relevant_songs, predicted_top_k, k)\n",
    "# recall = recall_at_k(actual_relevant_songs, predicted_top_k, k)\n",
    "# map_k = mean_average_precision(actual_relevant_songs, predicted_top_k, k)\n",
    "# mrr = mean_reciprocal_rank([song_id in actual_relevant_songs for song_id in predicted_top_k])\n",
    "# ndcg_k = ndcg_at_k(actual_relevant_songs, predicted_top_k, k)\n",
    "\n",
    "# # Output the additional metrics\n",
    "# print(f\"Precision@{k}: {precision:.4f}\")\n",
    "# print(f\"Recall@{k}: {recall:.4f}\")\n",
    "# print(f\"MAP@{k}: {map_k:.4f}\")\n",
    "# print(f\"MRR: {mrr:.4f}\")\n",
    "# print(f\"NDCG@{k}: {ndcg_k:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b23956e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a090827-73fc-4b1d-b427-1d53a4571eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
