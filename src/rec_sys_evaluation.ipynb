{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acc4e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5574c0f8",
   "metadata": {},
   "source": [
    "## Function Overview: `classify_time_of_day`\n",
    "\n",
    "**Purpose**:\n",
    "- Classifies a given timestamp into a time of day category based on the hour.\n",
    "\n",
    "**Parameters**:\n",
    "- `timestamp`: A datetime object containing the time information.\n",
    "\n",
    "**Returns**:\n",
    "- `1` for morning (4 AM to 11:59 AM).\n",
    "- `2` for afternoon to evening (12 PM to 7:59 PM).\n",
    "- `3` for night (8 PM to 3:59 AM).\n",
    "\n",
    "**Usage**:\n",
    "This function can be used to categorize activities or events into time-of-day segments for analysis or reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b0515e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_time_of_day(timestamp):\n",
    "    hour = timestamp.hour\n",
    "    if 4 <= hour < 12:\n",
    "        return 1\n",
    "    elif 12 <= hour < 20:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e523240",
   "metadata": {},
   "source": [
    "## Data Handling Process\n",
    "\n",
    "1. **Load Metadata**:\n",
    "   - The metadata is loaded from `id_metadata.csv`, using tab (`'\\t'`) as the delimiter, into the `metadata_df` DataFrame.\n",
    "\n",
    "2. **Load Listening History**:\n",
    "   - Listening history data is loaded from `lh_reduced.csv` into the `df` DataFrame.\n",
    "\n",
    "3. **Convert Timestamps**:\n",
    "   - The `timestamp` column in `df` is converted to datetime format to facilitate time-based operations.\n",
    "\n",
    "4. **Rename Columns**:\n",
    "   - In `metadata_df`, the column named `id` is renamed to `song` to align with the naming in `df`, facilitating easier merging or joining operations in future steps.\n",
    "\n",
    "5. **Classify Time of Day**:\n",
    "   - The `timestamp` column in `df` is used to apply the `classify_time_of_day` function, which categorizes timestamps into three time of day segments (morning, afternoon to evening, and night). The result is stored in a new column `time_of_day`.\n",
    "\n",
    "6. **Output DataFrame**:\n",
    "   - The final DataFrame `df`, now enhanced with the `time_of_day` categorization, is displayed or utilized for subsequent analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea651551",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = '../data/id_metadata.csv'\n",
    "listening_history_path = '../data/listening_history.csv'\n",
    "metadata_df = pd.read_csv(metadata_path, delimiter='\\t')\n",
    "df = pd.read_csv(listening_history_path, delimiter='\\t')\n",
    "\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "metadata_df.rename(columns={'id': 'song'}, inplace=True)\n",
    "df['time_of_day'] = df['timestamp'].apply(classify_time_of_day)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a3c29b",
   "metadata": {},
   "source": [
    "## Data Filtering Process Based on Recent Dates\n",
    "\n",
    "1. **Determine Latest Date**:\n",
    "   - Calculate the most recent date (`latest_date`) in the `timestamp` column of the DataFrame `df`.\n",
    "\n",
    "2. **Compute Date for One Week Ago**:\n",
    "   - Subtract 7 days from the `latest_date` using `pd.Timedelta`, resulting in the date `one_week_ago`.\n",
    "\n",
    "3. **Filter Recent Data**:\n",
    "   - Restrict `df` to only include rows where the `timestamp` is on or after `one_week_ago`, effectively filtering the data to the last week.\n",
    "\n",
    "4. **Output Filtered DataFrame**:\n",
    "   - The resulting DataFrame `df` now contains only the records from the past week, ready for analysis or further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84db4601",
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_date = df['timestamp'].max()\n",
    "one_week_ago = latest_date - pd.Timedelta(days=7)\n",
    "df = df[df['timestamp'] >= one_week_ago]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5137ce7",
   "metadata": {},
   "source": [
    "## Data Processing \n",
    "\n",
    "1. **Merge DataFrames**:\n",
    "   - `df` is merged with selected columns from `metadata_df` based on the 'song' column. The selected columns include song attributes such as release year, popularity, danceability, energy, key, mode, valence, and tempo.\n",
    "\n",
    "2. **Standardize Numeric Data**:\n",
    "   - Identify numeric columns in the merged DataFrame using `select_dtypes`.\n",
    "   - Standardize these numeric columns using `StandardScaler` to normalize the data, aiding in model performance and stability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83b395",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df, metadata_df[\n",
    "    ['song', 'release', 'popularity', 'danceability', 'energy', 'key', 'mode', 'valence', 'tempo']], on='song')\n",
    "\n",
    "numeric_cols_df = df.select_dtypes(include=np.number).columns\n",
    "sscaler = StandardScaler()\n",
    "df[numeric_cols_df] = sscaler.fit_transform(df[numeric_cols_df])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9611a5b",
   "metadata": {},
   "source": [
    "## DataFrame Operations Overview\n",
    "\n",
    "1. **Check for Missing Values**:\n",
    "   - `df.isnull().sum()` calculates the total number of missing values in each column of the DataFrame.\n",
    "   \n",
    "2. **Identify Unique Entries**:\n",
    "   - `df.song.unique()` retrieves an array of unique song IDs from the `song` column.\n",
    "   - `df.user.unique()` retrieves an array of unique user IDs from the `user` column.\n",
    "\n",
    "3. **Calculate Song Popularity**:\n",
    "   - The popularity of each song is calculated as the frequency of the song's appearance in the DataFrame divided by the total number of unique songs.\n",
    "   - This calculated popularity is then mapped back to the `song` column of `df` and stored in a new column `song_popularity`.\n",
    "\n",
    "The final DataFrame `df` is enhanced with a new `song_popularity` column which provides a relative measure of how frequently each song appears in the dataset, adjusted by the number of unique songs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69657202",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5581dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_names_song = df.song.unique()\n",
    "unique_names_user = df.user.unique()\n",
    "unique_names_song.shape, unique_names_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba74e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()\n",
    "song_popularity = df['song'].value_counts() / len(unique_names_song)\n",
    "df['song_popularity'] = df['song'].map(song_popularity)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4389497",
   "metadata": {},
   "source": [
    "## Building Interaction Matrix and Enriched Data\n",
    "\n",
    "### 1. **Initialize Interaction Matrix**:\n",
    "   - An interaction matrix is created with dimensions corresponding to the unique count of users and songs, initialized with zeros.\n",
    "\n",
    "### 2. **Map Users and Songs to Matrix Indices**:\n",
    "   - Dictionaries `user_indices` and `song_indices` are created to map user and song identifiers to matrix indices for easy access.\n",
    "\n",
    "### 3. **Populate Interaction Matrix**:\n",
    "   - Iterate through the DataFrame `df`, using mapped indices to fill the matrix with the logarithm of song popularity incremented by one, to factor in popularity dynamics in interactions.\n",
    "\n",
    "### 4. **Map Song Features**:\n",
    "   - Extract and map song-related features like release date, popularity, danceability, and others into dictionaries from `df`, indexed by song ID.\n",
    "\n",
    "### 5. **Prepare Data for Detailed Interaction DataFrame**:\n",
    "   - Arrays are prepared for user IDs, song IDs, song features, and interaction values by iterating over the interaction matrix and mapping features for each song-user pair.\n",
    "\n",
    "### 6. **Construct Feature-Rich DataFrame**:\n",
    "   - A new DataFrame `interaction_df` is created to encapsulate user IDs, song IDs, their interactions, and all the additional song features such as release, popularity, and more.\n",
    "\n",
    "This process effectively creates a structured and detailed view of user-song interactions, which is essential for tasks like recommendation systems or user behavior analysis based on music preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d473387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "interaction_matrix = np.zeros((df['user'].nunique(), len(unique_names_song)))\n",
    "\n",
    "# Map users and songs to matrix indices\n",
    "user_indices = {user: idx for idx, user in enumerate(df['user'].unique())}\n",
    "song_indices = {song: idx for idx, song in enumerate(unique_names_song)}\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    user_idx = user_indices[row['user']]\n",
    "    song_idx = song_indices[row['song']]\n",
    "    interaction_matrix[user_idx, song_idx] = np.log(row['song_popularity'] + 1)\n",
    "\n",
    "# Prepare dictionaries to map song IDs to their features\n",
    "song_features = {\n",
    "    'release': df.set_index('song')['release'].to_dict(),\n",
    "    'popularity': df.set_index('song')['popularity'].to_dict(),\n",
    "    'danceability': df.set_index('song')['danceability'].to_dict(),\n",
    "    'energy': df.set_index('song')['energy'].to_dict(),\n",
    "    'key': df.set_index('song')['key'].to_dict(),\n",
    "    'mode': df.set_index('song')['mode'].to_dict(),\n",
    "    'valence': df.set_index('song')['valence'].to_dict(),\n",
    "    'tempo': df.set_index('song')['tempo'].to_dict(),\n",
    "    'time_of_day': df.set_index('song')['time_of_day'].to_dict(),\n",
    "}\n",
    "\n",
    "# Create lists for DataFrame including additional features\n",
    "user_ids, song_ids, releases, popularities, danceabilities, energies, keys, modes, valences, tempos, interactions, time_of_days = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "for user in user_indices:\n",
    "    for song in song_indices:\n",
    "        user_ids.append(user_indices[user])\n",
    "        song_ids.append(song_indices[song])\n",
    "        interactions.append(interaction_matrix[user_indices[user], song_indices[song]])\n",
    "        # Map each song to its additional features\n",
    "        releases.append(song_features['release'][song])\n",
    "        popularities.append(song_features['popularity'][song])\n",
    "        danceabilities.append(song_features['danceability'][song])\n",
    "        energies.append(song_features['energy'][song])\n",
    "        keys.append(song_features['key'][song])\n",
    "        modes.append(song_features['mode'][song])\n",
    "        valences.append(song_features['valence'][song])\n",
    "        tempos.append(song_features['tempo'][song])\n",
    "        time_of_days.append(song_features['time_of_day'][song])\n",
    "\n",
    "# Create the interaction DataFrame\n",
    "interaction_df = pd.DataFrame({\n",
    "    'user_id': user_ids,\n",
    "    'song_id': song_ids,\n",
    "    'release': releases,\n",
    "    'popularity': popularities,\n",
    "    'danceability': danceabilities,\n",
    "    'energy': energies,\n",
    "    'key': keys,\n",
    "    'mode': modes,\n",
    "    'valence': valences,\n",
    "    'tempo': tempos,\n",
    "    'time_of_day': time_of_days,\n",
    "    'interaction': interactions\n",
    "})\n",
    "\n",
    "interaction_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b639379",
   "metadata": {},
   "source": [
    "## Data Encoding and Basic Statistics\n",
    "\n",
    "### 1. **Encode User and Song Identifiers**:\n",
    "   - `LabelEncoder` is used to transform non-numeric user and song identifiers into numeric representations. The transformed identifiers are stored in new columns `user_id` and `song_id` in the DataFrame `df`.\n",
    "\n",
    "### 2. **Calculate Unique Counts**:\n",
    "   - Calculate the number of unique users (`N`) and the number of unique songs (`M`) from the newly encoded `user_id` and `song_id` columns.\n",
    "\n",
    "This process provides a foundational step in preparing the dataset for more complex analytical tasks, such as modeling user-song interactions in recommendation systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c5b1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = LabelEncoder()\n",
    "song_encoder = LabelEncoder()\n",
    "df['user_id'] = user_encoder.fit_transform(df['user'])\n",
    "df['song_id'] = song_encoder.fit_transform(df['song'])\n",
    "\n",
    "N = df.user_id.nunique()  # Number of users\n",
    "M = df.song_id.nunique()  # Number of songs\n",
    "\n",
    "print(N, M)\n",
    "print(df.shape, interaction_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b332bb9c",
   "metadata": {},
   "source": [
    "## Data Splitting and Exploration\n",
    "\n",
    "### 1. **Split Data into Training and Testing Sets**:\n",
    "   - The `interaction_df` DataFrame is split into training (`df_train`) and testing sets (`df_test`) using a 20% test size allocation and a random seed for reproducibility.\n",
    "\n",
    "### 2. **Evaluate Unique Interaction Values**:\n",
    "   - Determine the number of unique interaction values within the `df_train` using `interaction.nunique()` to understand the diversity of user-song interactions.\n",
    "\n",
    "### 3. **Isolate Continuous Features**:\n",
    "   - Extract continuous features (columns from the third to the second-last) from both `df_train` and `df_test` into `continuous_data_train` and `continuous_data_test` respectively.\n",
    "   - Print shapes of the continuous datasets and `df_train` to understand their dimensions.\n",
    "\n",
    "### 4. **Check for Missing Values**:\n",
    "   - Calculate and display the total count of missing values per column in both `df_train` and `df_test` using `isnull().sum()` to assess data cleanliness and readiness for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36caace5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_test = train_test_split(interaction_df, test_size=0.2, random_state=42)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1424fcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.interaction.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf44d807",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_data_train = df_train.iloc[:, 2:-1]\n",
    "continuous_data_test = df_test.iloc[:, 2:-1]\n",
    "print(continuous_data_train.shape, continuous_data_test.shape, df_train.shape)\n",
    "continuous_data_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6338d72f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dfc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4219e54",
   "metadata": {},
   "source": [
    "## Model Loading and Inspection\n",
    "\n",
    "### 1. **Load Pre-trained Model**:\n",
    "   - The Keras model is loaded from a specified path (`model_path`), where it was previously saved as `model.h5`.\n",
    "\n",
    "### 2. **Display Model Architecture**:\n",
    "   - Use `model.summary()` to print the structure of the model. This includes details of all layers, their types, outputs, and the number of parameters both trainable and non-trainable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1d4c8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_path = '../helpers/model.h5'\n",
    "model = keras.models.load_model(model_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95e45ce",
   "metadata": {},
   "source": [
    "## Predictive Model Input Preparation and Execution\n",
    "\n",
    "### 1. **Define Continuous Features**:\n",
    "   - Specify the list of continuous features such as `release`, `popularity`, `danceability`, `energy`, `key`, `mode`, `valence`, `tempo`, and `time_of_day`, which are critical for the model’s input.\n",
    "\n",
    "### 2. **Select Specific User**:\n",
    "   - A specific user (`specific_user_id`) is selected from the test data to focus the predictions on.\n",
    "\n",
    "### 3. **Prepare User Input Array**:\n",
    "   - Create an array `user_input` where the selected user’s ID is repeated for each song, ensuring each song is paired with the user for prediction purposes.\n",
    "\n",
    "### 4. **Prepare Song Input Array**:\n",
    "   - Generate `song_input` as an array of indices representing all unique songs (`M` is the total count of unique songs).\n",
    "\n",
    "### 5. **Map Song IDs to DataFrame Indices**:\n",
    "   - Construct a dictionary `song_id_to_index` that maps each song ID to its corresponding index in `interaction_df` for efficient data retrieval.\n",
    "\n",
    "### 6. **Prepare Continuous Data for Model Input**:\n",
    "   - Use the mapping from song IDs to fetch and structure the continuous data corresponding to all songs, ensuring the data aligns correctly with each song ID in the input array.\n",
    "\n",
    "### 7. **Execute Predictions**:\n",
    "   - Make predictions for all songs for the selected user using the prepared inputs (`user_input`, `song_input`, and `continuous_data_input`). \n",
    "\n",
    "### 8. **Adjust Predictions**:\n",
    "   - Normalize predicted interactions by adding the mean interaction value (`mu`) from the training set to each prediction, compensating for any baseline shifts in interaction levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1e31f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_ids_test = df_test.user_id.values\n",
    "song_ids_test = df_test.song_id.values\n",
    "continuous_features = [\"release\", \"popularity\", \"danceability\", \"energy\", \"key\", \"mode\", \"valence\", \"tempo\",\n",
    "                       \"time_of_day\"]\n",
    "\n",
    "# Select a specific user for the prediction\n",
    "specific_user_id = user_ids_test[0]\n",
    "\n",
    "# Prepare input data for the model\n",
    "M = interaction_df['song_id'].nunique()  # Total number of unique songs\n",
    "user_input = np.array([specific_user_id] * M)  # Repeat the user ID for each song\n",
    "song_input = np.array(range(M))  # Array of all unique song IDs\n",
    "\n",
    "# Map song IDs to indices in df\n",
    "song_id_to_index = {id: idx for idx, id in enumerate(interaction_df['song_id'].unique())}\n",
    "\n",
    "# Prepare continuous data for all songs\n",
    "continuous_data_input = np.array(\n",
    "    [interaction_df.loc[song_id_to_index[song_id], continuous_features] for song_id in song_input])\n",
    "\n",
    "# Make predictions for this user with all songs, including the continuous data\n",
    "predicted_interactions = model.predict([user_input, song_input, continuous_data_input])\n",
    "\n",
    "mu = df_train.interaction.mean()  # Mean interaction value for normalization\n",
    "predicted_interactions = predicted_interactions.flatten() + mu\n",
    "predicted_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3544a5d2",
   "metadata": {},
   "source": [
    "## Extracting Top Song Recommendations\n",
    "\n",
    "### 1. **Determine Number of Recommendations**:\n",
    "   - Define `N` as 10 to specify the number of top recommendations to be retrieved.\n",
    "\n",
    "### 2. **Identify Top Recommendations**:\n",
    "   - Use `np.argsort()` on the `predicted_interactions` array to get indices of songs sorted by predicted interaction strength.\n",
    "   - Reverse the order (`[::-1]`) to start with the highest values and select the top `N` indices (`top_n_indices`).\n",
    "\n",
    "### 3. **Map Indices to Original Song IDs**:\n",
    "   - Convert the top indices (`top_n_indices`) back to original song IDs using the `song_encoder.inverse_transform()` function, yielding `top_n_song_ids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d086aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Determine the number of top recommendations top 10\n",
    "N = 10\n",
    "top_n_indices = np.argsort(predicted_interactions)[::-1][:N]\n",
    "\n",
    "# Convert the indices to original song IDs\n",
    "top_n_song_ids = song_encoder.inverse_transform(top_n_indices)\n",
    "\n",
    "print(f\"Top {N} recommended song IDs for user {specific_user_id} are:\", top_n_song_ids)\n",
    "print(user_input.shape)\n",
    "print(song_input.shape)\n",
    "print(continuous_data_input.shape)\n",
    "print(df[df.user_id == specific_user_id].song.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d0748d",
   "metadata": {},
   "source": [
    "## Displaying Detailed Song Information for Recommendations\n",
    "\n",
    "### 1. **Load Song Metadata**:\n",
    "   - Import song metadata from `id_information.csv` using `pd.read_csv()`, specifying a tab (`'\\t'`) as the delimiter, into the DataFrame `id_information`.\n",
    "\n",
    "### 2. **Filter Relevant Song Details**:\n",
    "   - Filter `id_information` to include only the entries corresponding to the `top_n_song_ids`, which are the IDs of the top recommended songs. This is accomplished using the `isin()` method, ensuring that only relevant song information is considered.\n",
    "\n",
    "### 3. **Display Top Recommended Songs**:\n",
    "   - Print details of the top recommended songs specifically tailored for the user (`specific_user_id`). The details displayed include the artist, song title, and album name from the `recommended_songs` DataFrame.\n",
    "   - This step highlights the song information, providing a more meaningful context to the recommendations, such as knowing the artist and album for each recommended song."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be590512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load song information\n",
    "id_information = pd.read_csv('../data/id_information.csv', sep='\\t')\n",
    "\n",
    "# Filter id_information to only include the top N recommended song IDs\n",
    "recommended_songs = id_information[id_information['id'].isin(top_n_song_ids)]\n",
    "\n",
    "# Print the details of the top N recommended songs\n",
    "print(f\"Top {N} recommended songs for user {specific_user_id} are:\")\n",
    "print(recommended_songs[['artist', 'song', 'album_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba3598a",
   "metadata": {},
   "source": [
    "## Analyzing and Displaying User's Recent Song History\n",
    "\n",
    "### 1. **Filter Songs for Specific User**:\n",
    "   - Extract rows from the DataFrame `df` where the `user_id` matches the specific user (`specific_user_id`). This subset contains all the songs interacted with by this particular user.\n",
    "\n",
    "### 2. **Sort Songs by Recent Play**:\n",
    "   - Sort the filtered data (`user_songs`) by the `timestamp` column in descending order to prioritize the most recent interactions. This sorted DataFrame is stored as `user_songs_sorted`.\n",
    "\n",
    "### 3. **Identify Last 5 Played Songs**:\n",
    "   - Retrieve the IDs of the last five songs played by this user from the top of the sorted DataFrame, ensuring these are the most recent songs interacted with.\n",
    "\n",
    "### 4. **Filter Song Metadata**:\n",
    "   - Use the song IDs (`last_5_song_ids`) to filter `id_information` to include only metadata for these last five songs. This step ensures that the information displayed pertains only to the most recent song interactions.\n",
    "\n",
    "### 5. **Display Song Information**:\n",
    "   - Print details about these last five songs, including the artist, song title, and album name, providing a comprehensive view of the user’s most recent music preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf2153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame for the specific user\n",
    "user_songs = df[df.user_id == specific_user_id]\n",
    "\n",
    "# Sort the data by the timestamp column in descending order\n",
    "user_songs_sorted = user_songs.sort_values(by='timestamp', ascending=False)\n",
    "\n",
    "last_5_song_ids = user_songs_sorted['song'].head(5).values\n",
    "print(last_5_song_ids)\n",
    "\n",
    "last_5_songs_info = id_information[id_information['id'].isin(last_5_song_ids)]\n",
    "print(\"Information about the last 5 songs played:\")\n",
    "print(last_5_songs_info[['artist', 'song', 'album_name']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123112a",
   "metadata": {},
   "source": [
    "## Similarity Calculation for Recommended Songs\n",
    "\n",
    "### 1. **Function for Similarity Calculation**:\n",
    "   - The `calculate_similarity` function computes the cosine similarity between a target song's feature vector and a set of song features. It returns the similarity scores from a 2D array to a 1D array for easier manipulation.\n",
    "\n",
    "### 2. **Initialize Variables**:\n",
    "   - Set `maxi` to -1 to track the highest similarity score found.\n",
    "   - Set `best_idx` to 0 to store the index of the song with the highest similarity.\n",
    "\n",
    "### 3. **Iterate Over Top Recommended Songs**:\n",
    "   - Loop through the first 10 recommended song IDs (`top_n_song_ids`).\n",
    "   - For each recommended song:\n",
    "     - Extract the feature vector from `interaction_df` using the song's index derived from `song_indices`.\n",
    "\n",
    "### 4. **Fetch Features of Last 5 Played Songs**:\n",
    "   - Construct an array of feature vectors for the last five played songs, again using indices from `interaction_df`.\n",
    "\n",
    "### 5. **Calculate Similarity for Each Recommended Song**:\n",
    "   - For each recommended song, compute its similarity to each of the last five played songs using the previously defined `calculate_similarity` function.\n",
    "   - Sum the similarity scores to get an overall similarity measure for the recommended song against all last played songs.\n",
    "\n",
    "### 6. **Identify Song with Highest Similarity**:\n",
    "   - Track and update the maximum similarity score (`maxi`) and the corresponding index (`best_idx`) if the current song's summed similarity score exceeds the previous maximum.\n",
    "\n",
    "This method effectively determines which of the top recommended songs are most similar to the user's recent listening habits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e61387",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(target_song_features, songs_features):\n",
    "    similarity = cosine_similarity([target_song_features], songs_features)\n",
    "    return similarity[0]  # similarity[0] because the result is in a 2D array\n",
    "\n",
    "maxi = -1\n",
    "best_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ddc80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    recommended_song_id = top_n_song_ids[i]\n",
    "    # Get the feature vector for the recommended song\n",
    "    recommended_song_features = interaction_df.loc[interaction_df.song_id[song_indices[recommended_song_id]]]\n",
    "\n",
    "    # Get the feature vectors for the last 5 played songs\n",
    "    last_5_songs_features = np.array(\n",
    "        [interaction_df.loc[interaction_df.song_id[song_indices[song_id]]] for song_id in last_5_song_ids])\n",
    "\n",
    "    # Calculate similarity\n",
    "    similarities = calculate_similarity(np.array(recommended_song_features).reshape(1, -1)[0], last_5_songs_features)\n",
    "    print(np.sum(similarities), end=\"\\n\\n\\n\")\n",
    "\n",
    "    if np.sum(similarities) > maxi:\n",
    "        maxi = np.sum(similarities)\n",
    "        best_idx = i"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127d798b",
   "metadata": {},
   "source": [
    "## Generating and Comparing Predicted Recommendations with Actual Preferences\n",
    "\n",
    "### 1. **Define Function to Retrieve Actual Relevant Songs**:\n",
    "   - The `get_relevant_songs` function is designed to fetch the unique song IDs that a specific user has interacted with from the DataFrame `df`. This establishes a baseline of songs that are known to be relevant to the user.\n",
    "\n",
    "### 2. **Initialize Prediction and Actual Dictionaries**:\n",
    "   - Two dictionaries, `predictions` and `actual`, are initialized to store the predicted top song IDs and actual relevant song IDs for each user, respectively.\n",
    "\n",
    "### 3. **Iterate Over a Subset of Users**:\n",
    "   - Loop through each user ID in the test set (limited to the first 10 users for demonstration). This looping facilitates the prediction and validation process for multiple users in a manageable subset.\n",
    "\n",
    "### 4. **Generate Predictions for Each User**:\n",
    "   - For each user:\n",
    "     - Create an input array (`user_input`) that repeats the user ID for each song, corresponding to the total number of unique songs (`M`).\n",
    "     - Prepare `continuous_data_input` by collecting the continuous feature data for all songs relevant to the current user input.\n",
    "     - Predict interaction scores using the model for all songs with the prepared inputs. Flatten the result to simplify handling.\n",
    "     - Sort the predicted scores in descending order and extract the indices of the top `N` scores.\n",
    "     - Use `song_encoder.inverse_transform` to convert these indices back into original song IDs (`top_n_song_ids`).\n",
    "\n",
    "### 5. **Store Predictions and Actual Song IDs**:\n",
    "   - Store the top `N` predicted song IDs for each user in the `predictions` dictionary.\n",
    "   - Fetch and store the actual relevant songs for the user using `get_relevant_songs` and store them in the `actual` dictionary.\n",
    "\n",
    "### 6. **Output Progress**:\n",
    "   - Print the total number of users being processed and the current progress after each user's data is processed to monitor the computation and ensure it is proceeding correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7dbcca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relevant_songs(user_id, df):\n",
    "    return df[df.user_id == user_id]['song'].unique()\n",
    "\n",
    "predictions = {}\n",
    "actual = {}\n",
    "\n",
    "i = 0\n",
    "# Get predictions and actual relevant songs for each user\n",
    "for user_id in user_ids_test[:500]:\n",
    "    # Predict top N songs\n",
    "    user_input = np.array([user_id] * M)\n",
    "    continuous_data_input = np.array(\n",
    "        [interaction_df.loc[song_id_to_index[song_id], continuous_features] for song_id in song_input])\n",
    "    predicted_interactions = model.predict([user_input, song_input, continuous_data_input]).flatten()\n",
    "    top_n_indices = np.argsort(predicted_interactions)[::-1][:N]\n",
    "    top_n_song_ids = song_encoder.inverse_transform(top_n_indices)\n",
    "\n",
    "    # Store the top N song IDs for each user\n",
    "    predictions[user_id] = top_n_song_ids.tolist()\n",
    "\n",
    "    # Get actual relevant songs for the user\n",
    "    actual_relevant_songs = get_relevant_songs(user_id, df)\n",
    "    actual[user_id] = actual_relevant_songs.tolist()\n",
    "    i += 1\n",
    "    print(len(user_ids_test), i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414032b",
   "metadata": {},
   "source": [
    "## Recommendation System Evaluation Metrics\n",
    "\n",
    "### Metric Calculations\n",
    "\n",
    "#### 1. **Precision at K**:\n",
    "   - Measures the proportion of recommended items in the top-K set that are relevant.\n",
    "\n",
    "#### 2. **Recall at K**:\n",
    "   - Assesses how many relevant items are found in the top-K recommendations.\n",
    "\n",
    "#### 3. **Mean Average Precision at K (MAP@K)**:\n",
    "   - Computes the mean of the average precision scores for each user, considering only the top-K recommendations.\n",
    "\n",
    "#### 4. **Mean Reciprocal Rank (MRR)**:\n",
    "   - Calculates the average of the reciprocal of the rank of the first relevant item among the recommendations.\n",
    "\n",
    "#### 5. **Normalized Discounted Cumulative Gain at K (NDCG@K)**:\n",
    "   - Evaluates the gain of a recommendation based on its position in the result list, giving higher importance to hits at top ranks.\n",
    "\n",
    "### Functions Defined\n",
    "\n",
    "- **`precision_at_k`**: Compares the top-K predicted items to the actual relevant items for each user to calculate precision.\n",
    "- **`recall_at_k`**: Identifies how many of the relevant items appear in the top-K predictions for each user.\n",
    "- **`mean_avg_precision_at_k`** and **`mean_average_precision_at_k`**: Both calculate the average precision at K for predictions against actual data.\n",
    "- **`mean_reciprocal_rank`**: Computes the average reciprocal rank where the rank is the position of the first relevant recommendation.\n",
    "- **`dcg_at_k`**: Computes the Discounted Cumulative Gain at K, a measure of ranking quality.\n",
    "- **`ndcg_at_k`**: Normalizes the DCG at K by the ideal or perfect DCG at K, providing a measure of the model's performance relative to the best possible scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af86dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_at_k(actual, predicted, k):\n",
    "    precision_scores = []\n",
    "    for user_id in actual:\n",
    "        # Initialize true positives count\n",
    "        true_positives = 0\n",
    "        # Check if user exists in predictions\n",
    "        if user_id in predicted and len(predicted[user_id]) >= k:\n",
    "            # Count the number of relevant items in the top k predictions\n",
    "            true_positives = len(set(predicted[user_id][:k]) & set(actual[user_id]))\n",
    "        # Calculate precision for this user\n",
    "        precision = true_positives / float(k)\n",
    "        precision_scores.append(precision)\n",
    "    # Return the average precision at k for all users\n",
    "    return sum(precision_scores) / len(precision_scores)\n",
    "\n",
    "\n",
    "def recall_at_k(actual, predicted, k):\n",
    "    recall_scores = []\n",
    "    for user_id in actual:\n",
    "        # Initialize true positives count\n",
    "        true_positives = 0\n",
    "        # Check if user exists in predictions\n",
    "        if user_id in predicted:\n",
    "            # Count the number of relevant items in the top k predictions\n",
    "            true_positives = len(set(predicted[user_id][:k]) & set(actual[user_id]))\n",
    "            recall = true_positives / float(len(actual[user_id]))\n",
    "        else:\n",
    "            # If no predictions for the user, recall is 0\n",
    "            recall = 0.0\n",
    "        recall_scores.append(recall)\n",
    "    # Return the average recall at k for all users\n",
    "    return sum(recall_scores) / len(recall_scores)\n",
    "\n",
    "\n",
    "def avg_precision_at_k(actual, predicted, k=10):\n",
    "    ap_sum = 0\n",
    "    for user, true_items in actual.items():\n",
    "        pred_items = predicted[user][:k]\n",
    "        hits = 0\n",
    "        sum_precs = 0\n",
    "        for i, p in enumerate(pred_items):\n",
    "            if p in true_items:\n",
    "                hits += 1\n",
    "                sum_precs += hits / (i + 1.0)\n",
    "        ap_sum += sum_precs / min(len(true_items), k)\n",
    "    return ap_sum / len(actual)\n",
    "\n",
    "\n",
    "def mean_avg_precision_at_k(actual, predicted, k=10):\n",
    "    return avg_precision_at_k(actual, predicted, k)\n",
    "\n",
    "\n",
    "def mean_average_precision_at_k(actual, predicted, k=10):\n",
    "    AP_sum = 0.0\n",
    "    for user_id in actual:\n",
    "        if user_id in predicted:\n",
    "            pred_items = predicted[user_id][:k]\n",
    "            hits = 0\n",
    "            sum_precisions = 0\n",
    "            for i, p in enumerate(pred_items):\n",
    "                if p in actual[user_id] and p not in pred_items[:i]:\n",
    "                    hits += 1\n",
    "                    sum_precisions += hits / (i + 1.0)\n",
    "            AP_sum += sum_precisions / min(len(actual[user_id]), k)\n",
    "    return AP_sum / len(actual)\n",
    "\n",
    "\n",
    "def mean_reciprocal_rank(actual, predicted):\n",
    "    MRR_sum = 0.0\n",
    "    for user_id in actual:\n",
    "        if user_id in predicted:\n",
    "            pred_items = predicted[user_id]\n",
    "            for rank, p in enumerate(pred_items, start=1):\n",
    "                if p in actual[user_id]:\n",
    "                    MRR_sum += 1.0 / rank\n",
    "                    break\n",
    "    return MRR_sum / len(actual)\n",
    "\n",
    "\n",
    "def dcg_at_k(relevances, k):\n",
    "    relevances = np.asfarray(relevances)[:k]\n",
    "    if relevances.size:\n",
    "        return np.sum(relevances / np.log2(np.arange(2, relevances.size + 2)))\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def ndcg_at_k(actual, predicted, k=10):\n",
    "    NDCG_sum = 0.0\n",
    "    for user_id in actual:\n",
    "        if user_id in predicted:\n",
    "            pred_items = predicted[user_id][:k]\n",
    "            true_relevances = [1 if item in actual[user_id] else 0 for item in pred_items]\n",
    "            ideal_relevances = [1] * len(actual[user_id])\n",
    "            NDCG_sum += dcg_at_k(true_relevances, k) / dcg_at_k(ideal_relevances, k)\n",
    "    return NDCG_sum / len(actual)\n",
    "\n",
    "k = 10\n",
    "precision = precision_at_k(actual, predictions, k)\n",
    "recall = recall_at_k(actual, predictions, k)\n",
    "map_k = mean_average_precision_at_k(actual, predictions, k)\n",
    "mrr = mean_reciprocal_rank(actual, predictions)\n",
    "ndcg_k = ndcg_at_k(actual, predictions, k)\n",
    "\n",
    "print(f\"Precision@{k}: {precision}\")\n",
    "print(f\"Recall@{k}: {recall}\")\n",
    "print(f\"MAP@{k}: {map_k}\")\n",
    "print(f\"MRR: {mrr}\")\n",
    "print(f\"NDCG@{k}: {ndcg_k}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e2f18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
